{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scnet1 import scnet50\n",
    "model = scnet50(pretrained=False)\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Lambda\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MEbeautyDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    SCUT-FBP5500 dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train=True, transform=None):\n",
    "        split_train = 'train2.txt'\n",
    "        split_test =  'test2.txt'\n",
    "        if train:\n",
    "            self.face_img = pd.read_csv(split_train, sep=' ',names=['image','score'], header=None).iloc[:, 0].tolist()\n",
    "            self.face_score = pd.read_csv(split_train,sep=' ',names=['image','score'], header=None).iloc[:,1].tolist()\n",
    "           # self.face_score = pd.read_csv(split_train,sep=' ',names=['image','score'], header=None).iloc[:,1].tolist()\n",
    "            #self.face_score=8.65\n",
    "            #self.face_score = [x - 1 for x in self.face_score]\n",
    "        else:\n",
    "            self.face_img = pd.read_csv(split_test,sep=' ',names=['image','score'], header=None).iloc[:, 0].tolist()\n",
    "            self.face_score = pd.read_csv(split_test, sep=' ', names=['image','score'], header=None).iloc[:, 1].tolist()\n",
    "            #self.face_score = [x - 1 for x in self.face_score]\n",
    "        self.transform = transform\n",
    "            \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.face_img)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #image = io.imread('/home/ubuntu/SCUT5500/Images/'+str(self.face_img[index]))\n",
    "        image=Image.open(self.face_img[index])\n",
    "        score = self.face_score[index]\n",
    "       # sample = {'image': image, 'score': score, 'class': round(score) - 1, 'filename': self.face_img[index]}\n",
    "\n",
    "        \n",
    "        if self.transform:\n",
    "           image = self.transform(image)\n",
    "        sample = image,  score\n",
    "        return sample\n",
    "\n",
    "\n",
    "def load_scutfbp5500_64():\n",
    "\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.RandomCrop(224),\n",
    "            transforms.RandomRotation(30),\n",
    "            # transforms.ColorJitter(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(MEbeautyDataset(train=True, transform=data_transforms['train']),\n",
    "                                               batch_size=128, shuffle=True, num_workers=50,\n",
    "                                               pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(MEbeautyDataset(train=False, transform=data_transforms['test']),\n",
    "                                              batch_size=128, shuffle=False, num_workers=50,\n",
    "                                              pin_memory=True)\n",
    "\n",
    "    return test_loader, train_loader\n",
    "        \n",
    "train,test = load_scutfbp5500_64()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 224, 224])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 3, 224, 224])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 3, 224, 224])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 3, 224, 224])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 3, 224, 224])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 3, 224, 224])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 3, 224, 224])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 3, 224, 224])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 3, 224, 224])\n",
      "torch.Size([128])\n",
      "torch.Size([108, 3, 224, 224])\n",
      "torch.Size([108])\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, targets) in enumerate(test):\n",
    "    print(inputs.shape)\n",
    "    print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(1.0528, grad_fn=<MseLossBackward>)\n",
      "tensor(4.4145, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1483, grad_fn=<MseLossBackward>)\n",
      "tensor(2.2665, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9390, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5076, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8424, grad_fn=<MseLossBackward>)\n",
      "tensor(2.1635, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7627, grad_fn=<MseLossBackward>)\n",
      "tensor(2.3710, grad_fn=<MseLossBackward>)\n",
      "1\n",
      "tensor(1.1988, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9556, grad_fn=<MseLossBackward>)\n",
      "tensor(1.2809, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9567, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4629, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8883, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9559, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9956, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9627, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6377, grad_fn=<MseLossBackward>)\n",
      "2\n",
      "tensor(2.8458, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9531, grad_fn=<MseLossBackward>)\n",
      "tensor(1.1688, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8065, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4231, grad_fn=<MseLossBackward>)\n",
      "tensor(2.8350, grad_fn=<MseLossBackward>)\n",
      "tensor(1.7360, grad_fn=<MseLossBackward>)\n",
      "tensor(2.2793, grad_fn=<MseLossBackward>)\n",
      "tensor(2.1913, grad_fn=<MseLossBackward>)\n",
      "tensor(3.2298, grad_fn=<MseLossBackward>)\n",
      "3\n",
      "tensor(1.0912, grad_fn=<MseLossBackward>)\n",
      "tensor(1.3963, grad_fn=<MseLossBackward>)\n",
      "tensor(2.5085, grad_fn=<MseLossBackward>)\n",
      "tensor(3.3300, grad_fn=<MseLossBackward>)\n",
      "tensor(2.5149, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7944, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8642, grad_fn=<MseLossBackward>)\n",
      "tensor(1.2960, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6449, grad_fn=<MseLossBackward>)\n",
      "tensor(3.5413, grad_fn=<MseLossBackward>)\n",
      "4\n",
      "tensor(2.1660, grad_fn=<MseLossBackward>)\n",
      "tensor(1.5184, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9127, grad_fn=<MseLossBackward>)\n",
      "tensor(0.8511, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load('model.pt'))\n",
    "#epoch=13\n",
    "for epoch in range(10):\n",
    "    print(epoch)\n",
    "    # enumerate mini batches\n",
    "    for i, (inputs, targets) in enumerate(train):\n",
    "# clear the gradients\n",
    "        #print(inputs.shape)\n",
    "        optimizer.zero_grad()\n",
    "# compute the model output\n",
    "        yhat = model(inputs)\n",
    "# calculate loss\n",
    "        yhat=yhat.reshape(yhat.shape[0])\n",
    "        loss = criterion(yhat, targets.float())\n",
    "        print(loss)\n",
    "        # credit assignment\n",
    "        loss.backward()\n",
    "# update model weights\n",
    "        optimizer.step()\n",
    "        torch.save(model.state_dict(), 'model_ME_crop.pt'+str(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2080, -0.2055, -0.0272, -0.2931, -0.3325, -0.1976, -0.1750, -0.2390,\n",
       "        -0.1254, -0.0871, -0.3926, -0.1047, -0.0020, -0.1189, -0.2423, -0.3211,\n",
       "        -0.2635, -0.3402, -0.0896, -0.2739, -0.1207, -0.1528, -0.1230, -0.1966,\n",
       "        -0.1613, -0.2073, -0.2206, -0.3550, -0.0750, -0.1872, -0.1096, -0.2785,\n",
       "        -0.1281, -0.2639, -0.1799, -0.2709, -0.1085, -0.1472, -0.1397, -0.2089,\n",
       "        -0.1146, -0.0718, -0.0262, -0.1870, -0.2033, -0.0842, -0.0651, -0.2398,\n",
       "        -0.1572,  0.0630,  0.0632, -0.3426, -0.2343, -0.1987, -0.3344, -0.0406,\n",
       "        -0.0389, -0.2417, -0.2236, -0.1940, -0.0938, -0.1367, -0.2414, -0.2096])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sg\n",
      "tensor([5.3100, 6.0552, 5.5541, 5.3976, 5.4677, 5.4268, 5.4425, 5.3095, 5.1990,\n",
      "        5.2820, 5.3688, 5.3299, 5.3044, 5.3797, 5.5159, 5.4599, 5.4524, 5.7326,\n",
      "        5.3793, 6.1364, 5.4062, 5.3747, 5.4155, 5.5827, 5.5760, 4.8446, 5.3563,\n",
      "        5.4740, 5.9389, 5.2457, 5.4738, 5.3186, 5.4336, 5.4216, 5.6408, 5.5344,\n",
      "        5.3970, 5.8780, 5.8875, 5.4189, 5.4165, 5.4617, 5.3994, 5.3361, 5.3892,\n",
      "        6.3663, 5.4011, 5.3104, 5.4498, 5.3936, 5.4053, 5.4063, 5.4659, 5.3907,\n",
      "        5.2229, 5.6066, 5.5606, 5.4358, 5.5617, 5.7181, 5.5118, 5.3973, 5.3989,\n",
      "        5.4151, 5.6274, 5.4526, 5.4128, 5.5230, 5.3675, 5.4967, 5.3810, 5.4365,\n",
      "        5.2971, 5.4557, 5.6005, 5.3804, 5.4054, 5.4337, 5.3817, 5.3957, 5.2248,\n",
      "        5.5874, 5.5754, 5.3138, 5.5024, 5.5282, 5.4788, 5.3912, 5.4625, 5.5450,\n",
      "        5.5346, 5.4656, 5.2072, 5.3630, 5.6817, 5.4476, 5.5374, 5.1221, 5.3852,\n",
      "        6.1180, 5.5582, 5.4454, 5.7007, 5.3597, 5.5140, 5.5053, 5.4000, 5.2792,\n",
      "        5.3306, 5.4819, 5.3873, 5.8356, 5.2858, 5.3264, 5.5480, 5.4176, 5.4133,\n",
      "        5.4074, 5.3064, 5.4338, 5.2770, 5.2847, 5.4110, 5.8699, 5.4482, 5.3284,\n",
      "        5.5282, 5.5636], grad_fn=<ViewBackward>)\n",
      "[[1.         0.04819536]\n",
      " [0.04819536 1.        ]]\n",
      "1.7340117747924093\n",
      "1.5257310057103297\n",
      "tensor([5.7782, 5.8131, 6.0507, 5.6317, 5.7974, 5.7403, 5.8230, 5.6671, 5.7892,\n",
      "        5.5923, 5.6348, 5.7166, 5.6920, 5.7379, 5.8936, 5.4340, 5.3088, 4.7746,\n",
      "        5.6976, 5.7916, 5.8213, 5.6431, 5.6839, 5.5124, 5.5861, 6.2312, 5.8172,\n",
      "        5.6743, 5.7555, 5.7161, 5.8883, 5.8248, 5.8593, 5.8459, 5.7659, 5.6911,\n",
      "        5.8494, 5.7184, 5.7504, 5.7736, 5.8235, 5.7534, 5.7351, 5.7854, 5.7476,\n",
      "        5.8191, 5.6738, 5.8709, 5.7317, 5.6109, 5.7091, 5.8764, 6.0450, 5.9539,\n",
      "        5.7206, 5.6881, 5.3770, 5.8425, 5.6592, 5.7145, 5.7494, 5.9280, 5.7795,\n",
      "        5.3294, 5.6954, 5.5294, 5.6869, 5.8155, 5.7287, 5.7772, 5.8636, 5.7564,\n",
      "        5.5201, 5.6912, 5.7636, 5.6167, 5.7721, 5.7971, 5.5923, 5.6759, 5.8533,\n",
      "        5.7929, 5.7965, 5.7614, 5.8593, 5.6230, 5.7951, 5.8161, 5.7358, 5.7295,\n",
      "        5.7497, 5.7186, 5.5794, 5.9435, 5.8073, 5.7729, 5.7947, 5.7564, 6.1914,\n",
      "        5.8481, 5.9039, 5.6618, 5.5590, 5.7903, 5.6099, 5.6449, 5.8899, 5.8601,\n",
      "        6.6181, 5.7308, 5.7593, 5.6853, 5.7310, 5.5227, 5.5935, 5.7146, 5.8430,\n",
      "        5.7141, 5.7886, 5.9175, 5.8846, 5.7173, 6.1205, 6.1236, 5.8723, 5.8283,\n",
      "        5.8953, 5.7228], grad_fn=<ViewBackward>)\n",
      "[[1.         0.11546311]\n",
      " [0.11546311 1.        ]]\n",
      "1.415652670814328\n",
      "1.235797601000932\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-2-30cfb8606a48>\", line 51, in __getitem__\n    image = self.transform(image)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 70, in __call__\n    img = t(img)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 175, in __call__\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 218, in normalize\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\nRuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-31556a0593c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m    \u001b[0;31m# acc = accuracy_score(actual, yhat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-31556a0593c6>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(test_dl, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m        \u001b[0;31m# if k==0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# evaluate the model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-2-30cfb8606a48>\", line 51, in __getitem__\n    image = self.transform(image)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 70, in __call__\n    img = t(img)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/transforms.py\", line 175, in __call__\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision/transforms/functional.py\", line 218, in normalize\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\nRuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_ME.pt4'))\n",
    "from numpy import vstack\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import sklearn\n",
    "import math\n",
    "print('sg')\n",
    "\n",
    "import numpy as np\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    k=0\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "       # if k==0:\n",
    "            # evaluate the model on the test set\n",
    "            yhat = model(inputs)\n",
    "            yhat=yhat.reshape(yhat.shape[0])\n",
    "            print(yhat)\n",
    "            # retrieve numpy array\n",
    "            yhat = yhat.detach().numpy()\n",
    "            actual = targets.numpy()\n",
    "           # actual = actual.reshape((len(actual), 1))\n",
    "            print(np.corrcoef(yhat,actual))\n",
    "            mse1= sklearn.metrics.mean_squared_error(yhat,actual) \n",
    "            mse1=math.sqrt(mse1)\n",
    "            print(mse1)\n",
    "            print(mae(yhat,actual))\n",
    "            # round to class values\n",
    "            #yhat = yhat.round()\n",
    "            # store\n",
    "            #predictions.append(yhat)\n",
    "            #actuals.append(actual)\n",
    "          #  k=k+1\n",
    "    #predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    #print(predictions)\n",
    "    #print(actuals)\n",
    "    # calculate accuracy\n",
    "   # acc = accuracy_score(actual, yhat)\n",
    "    return acc\n",
    "print(evaluate_model(test, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
